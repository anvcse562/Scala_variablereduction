{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define the target variable and input dataset. Ensure that the input variables are assigned before executing this script.\n",
    "// This automation script generates two tables. It first calculates the relevancy (individual variable contribution to the response), followed by the interaction of input variables using Random Forest.\n",
    "// 1. The importance of all variables will be saved in the table hiveschema.FinalVariableImportance.\n",
    "// 2. Next, we retrieve the top 250 variables from the first step, build a Random Forest model, and list the variable importances in descending order in the table schema.final_var_importancev.\n",
    "val target1=\"target\" \n",
    "val tablename=\"hivetablename\"\n",
    "val username=\"userschema\"\n",
    "val numberofvar=\"50\"   // This parameter limits the number of variables to Random Forest\n",
    "// sampling  method - oversampling\n",
    "val Target_1=20000\n",
    "val Target_0=40000\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " //don't modify anything below unless needed\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.{ArrayBuffer, Map}\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.feature.ChiSqSelector\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.Interaction\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "\n",
    "\n",
    "//don't modify anything below\n",
    " def rankCategory(categoryFrequency: scala.collection.Map[String,Long]) : scala.collection.Map[String,Int] = {\n",
    "\n",
    "     val frequency =  categoryFrequency.map(_._2).toSeq.distinct.sorted\n",
    "     val freqIndex = frequency.zipWithIndex.toMap\n",
    "     val categoryRanked : scala.collection.Map[String,Int] = categoryFrequency.map{ case (category, frequency) =>\n",
    "       (category,freqIndex.getOrElse(frequency,0))\n",
    "     }\n",
    "     categoryRanked\n",
    "   }\n",
    "\n",
    "   def convertDataFrameToWordFrequency(inputDF : DataFrame) : DataFrame = {\n",
    "     var outputDataFrame = inputDF\n",
    "     val schema =  inputDF.schema\n",
    "     schema.filter(_.dataType == StringType).foreach( structField => {\n",
    "       val colName = structField.name\n",
    "       val numOfCategories  =  outputDataFrame.select(colName).distinct().count()\n",
    "       if(numOfCategories > 80){\n",
    "         // delete the column object\n",
    "         outputDataFrame = outputDataFrame.drop(colName)\n",
    "       }\n",
    "       else{\n",
    "        val categoryFrequency : scala.collection.Map[String,Long] =  outputDataFrame.select(colName).groupBy(colName).agg(count(\"*\").alias(\"count\")).rdd.\n",
    "           map{case row => (row.getAs[String](colName), row.getAs[Long](\"count\"))}.collectAsMap()\n",
    "        val categoryWithRank  : scala.collection.Map[String,Int] = rankCategory(categoryFrequency)\n",
    "        val convertStringToRankUDF = udf[Int,String]((s:String) =>{\n",
    "          categoryWithRank.getOrElse(Option(s).getOrElse(\"\"),-1)\n",
    "        })\n",
    "         // converts string to their rank(Integer)\n",
    "         outputDataFrame = outputDataFrame.\n",
    "           withColumn(colName,convertStringToRankUDF(col(colName)) )\n",
    "       }\n",
    "     })\n",
    "     outputDataFrame\n",
    "   }\n",
    "\n",
    "  //val inputDataFrame1 = spark.sql(s\"select * from (select  cast( $target1 as int)  as target1,a.* from  $tablename a where  cast( $target1 as int)=1 limit 50000 )a1 union  select * from (select  cast( $target1 as int)  as target1,a.* from  $tablename a where  cast( $target1 as int)=0 limit 100000 )a2\") \n",
    "  val inputDataFrame1 = spark.sql(s\"select  cast( $target1 as int)  as target1,a.* from  $tablename a where  cast( $target1 as int)=1  \")\n",
    "  val inputDataFrame2 = spark.sql(s\"select  cast( $target1 as int)  as target1,a.* from  $tablename a where  cast( $target1 as int)=0  \")\n",
    "  def getRandom (inputDF : DataFrame, n : Int) : DataFrame = {\n",
    "    val count = inputDF.count();\n",
    "    val howManyTake = if (count > n) n else count;\n",
    "    val outputDataFrame1=inputDF.sample(true,1.0*howManyTake/count).limit (n)\n",
    "    outputDataFrame1\n",
    "   }\n",
    "   val interim1=getRandom(inputDataFrame1,s\"$Target_1\".toInt)\n",
    "   val interim2=getRandom(inputDataFrame1,s\"$Target_0\".toInt)\n",
    "   interim2.count\n",
    "   val inputDataFrame1=interim1.union(interim2)\n",
    "   inputDataFrame1.select(inputDataFrame1.columns.filter(_.contains(\"id\",\"key\",\"ID\",\"KEY\")).map(inputDataFrame1(_)) : _*).drop()\n",
    "   inputDataFrame1.select(inputDataFrame1.columns.filter(_.startsWith(\"id\")).map(inputDataFrame1(_)) : _*).drop()\n",
    "   inputDataFrame1.select(inputDataFrame1.columns.filter(_.endsWith(\"id\")).map(inputDataFrame1(_)) : _*).drop()\n",
    "   \n",
    "   \n",
    "      val count1=inputDataFrame1.count\n",
    "      var inputDataFrame=spark.emptyDataFrame\n",
    "      if (count1 >= 10000000){\n",
    "       inputDataFrame = inputDataFrame1.sample(false, 0.02)\n",
    "      } else if(count1>5000000 & count1 <10000000){\n",
    "         inputDataFrame=inputDataFrame1.sample(false,0.04)\n",
    "      } else if(count1 <5000000 & count1>2000000){\n",
    "         inputDataFrame=inputDataFrame1.sample(false,0.06)\n",
    "      } else if(count1 <2000000 & count1>500000){\n",
    "         inputDataFrame=inputDataFrame1.sample(false,0.3)\n",
    "      } else if(count1 >200000 & count1 <=500000) {\n",
    "      println(\"hi\")\n",
    "         inputDataFrame=inputDataFrame1.sample(false,0.8)\n",
    "         inputDataFrame.cache()\n",
    "         inputDataFrame.count()\n",
    "      } else {\n",
    "       println(\"hi\")\n",
    "       inputDataFrame=inputDataFrame1\n",
    "      }\n",
    "    inputDataFrame.count()\n",
    "    \n",
    "      val dataFrameWithCharRank = convertDataFrameToWordFrequency(inputDataFrame)\n",
    "       dataFrameWithCharRank.show()\n",
    "\n",
    "       val fields = dataFrameWithCharRank.schema.fields filter {\n",
    "       x => x.dataType match { \n",
    "             case x: org.apache.spark.sql.types.DateType => true\n",
    "             case x: org.apache.spark.sql.types.TimestampType => true\n",
    "             case _ => false \n",
    "             } \n",
    "           } map { x => x.name }\n",
    "\n",
    "       val newDf = fields.foldLeft(dataFrameWithCharRank){ case(dframe,field) => dframe.drop(field) }\n",
    "           var newDf1 = newDf.drop(\"target\")\n",
    "       var output3 = \"\"\n",
    "       val a=newDf1.columns.foreach(a => output3+=\"corr(\"+a+\",target1) as \"+a+\",\")\n",
    "       newDf1.registerTempTable(\"variablereduction\")\n",
    "       var b=output3.dropRight(1)\n",
    "        val varimportance=spark.sql(s\"select $b from variablereduction\")\n",
    "          val schema1 =  varimportance.schema\n",
    "          val outPutList = ArrayBuffer.empty[String] \n",
    "          schema1.foreach( structField1 => {\n",
    "          var outPutLine = \"\"\n",
    "          val colName = structField1.name\n",
    "          //val numOfCategories1  =  varimportance.map{x => (x.getAs[Double](\"colName\"))}\n",
    "          val numOfCategories1  =  varimportance.select(colName).collectAsList.get(0).toString\n",
    "          outPutLine += colName + \"|\" + numOfCategories1\n",
    "          outPutList += outPutLine\n",
    "          })\n",
    "          \n",
    "        val hadoopConf=sc.hadoopConfiguration\n",
    "\n",
    "       var hdfs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n",
    "       hdfs.delete(new org.apache.hadoop.fs.Path(s\"/user/$username/varimp\"))\n",
    "\n",
    "       sc.parallelize(List(outPutList.mkString(\"\\n\").replaceAll(\"[\\\\[\\\\]]\", \"\"))).repartition(1).saveAsTextFile(s\"/user/$username/varimp\")\n",
    "        val textfile1=sc.textFile(s\"/user/$username/varimp\")\n",
    "        spark.sql(s\"drop table if exists $username.FinalVariableIMportance\")  \n",
    "         textfile1.map(x => {val cols=x.split(\"\\\\|\", -1)\n",
    "                            (cols(0),cols(1))}).toDF(\"variablenames\",\"RankVI\").write.saveAsTable(s\"$username.FinalVariableIMportance\")\n",
    "        \n",
    "       val variablefinallist=spark.sql(s\"select b.variablenames from (select row_number() over ( order by c.RankVI1 Desc) as rowid,c.* from (select abs(cast(RankVI as decimal(18,12))) as RankVI1 , a.* from $username.FinalVariableIMportance a) c where UPPER(c.variablenames) not in('TARGET','HOUSEKEY','TARGET1') )B where  RankVI<=0.8 and rowid<=$numberofvar\").toDF.collect.toArray.map(_.get(0)).map(value => if(value != null) value.toString() else value).map(_.toString)\n",
    "\n",
    "        \n",
    "       val nonmissing=newDf1.na.fill(-1)\n",
    "           val indexer = new StringIndexer().setInputCol(\"target1\").setOutputCol(\"label\")\n",
    "         val assembler = new VectorAssembler().setInputCols(variablefinallist).setOutputCol(\"features\")\n",
    "          val output = assembler.transform(nonmissing)\n",
    "\n",
    "           val labelIndexer = new StringIndexer().setInputCol(\"target1\").setOutputCol(\"indexedLabel\").fit(output)\n",
    "       val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(output)\n",
    "        val Array(trainingData, testData) = output.randomSplit(Array(0.7, 0.3))\n",
    "       val rf = new RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setNumTrees(10).setFeatureSubsetStrategy(\"auto\").setImpurity(\"gini\").setMaxDepth(4).setMaxBins(32)\n",
    "\n",
    "       val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "        val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))\n",
    "        val model = pipeline.fit(trainingData)\n",
    "        val predictions = model.transform(testData)\n",
    "         val Tpredictions = model.transform(trainingData)\n",
    "       predictions.select(\"predictedLabel\", \"target1\", \"features\").show(5)\n",
    "       val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "       val precision1 =new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "       val roc_evaluator = new BinaryClassificationEvaluator().setLabelCol(\"target1\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\").evaluate(predictions)\n",
    "       val roc_evaluator1 = new BinaryClassificationEvaluator().setLabelCol(\"target1\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\").evaluate(Tpredictions)\n",
    "\n",
    "       val accuracy = evaluator.evaluate(predictions)\n",
    "       val accuracy1 = evaluator.evaluate(Tpredictions)\n",
    "       println(\"Test Error = \" + (1.0 - accuracy))\n",
    "       println(\"Training Error = \" + (1.0 - accuracy1))\n",
    "        println(\"precision =\"+ precision1)\n",
    "         println(\"precision =\"+ roc_evaluator)\n",
    "           println(\"precision =\"+ roc_evaluator1)\n",
    "       val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]\n",
    "        println(\"Learned classification forest model:\\n\" + rfModel.toDebugString)\n",
    "        var varimpvalues  = model.stages(2).asInstanceOf[RandomForestClassificationModel].featureImportances.toArray\n",
    "        var rfModel1  = model.stages(2).asInstanceOf[RandomForestClassificationModel].featureImportances.size\n",
    "       var indexVar = 0\n",
    "       val outputlistfinal1 = ArrayBuffer.empty[String] \n",
    "        while(indexVar < rfModel1) {\n",
    "       var outputlist3 = \"\"\n",
    "       outputlist3+=variablefinallist(indexVar)+\"|\"+varimpvalues(indexVar)\n",
    "       outputlistfinal1+=outputlist3\n",
    "       indexVar = indexVar + 1\n",
    "        }\n",
    "        spark.sql(s\"drop table if exists $username.final_var_importance\")  \n",
    "                val hadoopConf=sc.hadoopConfiguration\n",
    "\n",
    "       var hdfs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n",
    "       hdfs.delete(new org.apache.hadoop.fs.Path(s\"/user/$username/final_var_importance\"))\n",
    "        \n",
    "        \n",
    "         sc.parallelize(List(outputlistfinal1.mkString(\"\\n\"))).repartition(1).saveAsTextFile(s\"/user/$username/final_var_importance\")\n",
    "          val textfile2=sc.textFile(s\"/user/$username/final_var_importance\") \n",
    "         textfile2.map(x => {val cols=x.split(\"\\\\|\", -1)\n",
    "                            (cols(0),cols(1))}).toDF(\"variablenames\",\"RandomF_VF\").write.mode(\"overwrite\").saveAsTable(s\"$username.final_var_importance\")\n",
    "\n",
    "         spark.sql(s\"select * from $username.final_var_importance order by  RandomF_VF desc \").show(40,false)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
